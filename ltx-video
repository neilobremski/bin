#!/bin/bash

# ltx-video: Wrapper script for generating videos with LTX-Video
# Usage: ltx-video [options] [prompt text...] [output.mp4]
# Options starting with - or -- are passed to inference.py
# File paths are used as reference images
# Text arguments become the prompt
# Last argument ending in .mp4 is the output filename

set -e

# Determine LTX-Video repo location early (for help to show available models)
if [ -d "$HOME/repos/LTX-Video" ]; then
  LTX_DIR="$HOME/repos/LTX-Video"
elif [ -d "$HOME/LTX-Video" ]; then
  LTX_DIR="$HOME/LTX-Video"
else
  LTX_DIR=""
fi

# Install function
do_install() {
  echo "=== LTX-Video Installation ==="
  
  # Determine install location
  if [ -d "$HOME/repos" ]; then
    INSTALL_DIR="$HOME/repos"
  else
    INSTALL_DIR="$HOME"
  fi
  
  TARGET_DIR="$INSTALL_DIR/LTX-Video"
  
  # Clone if not already present
  if [ -d "$TARGET_DIR" ]; then
    echo "LTX-Video repo already exists at $TARGET_DIR"
    echo "Pulling latest changes..."
    cd "$TARGET_DIR"
    git pull
  else
    echo "Cloning LTX-Video to $TARGET_DIR..."
    git clone https://github.com/Lightricks/LTX-Video.git "$TARGET_DIR"
    cd "$TARGET_DIR"
  fi
  
  # Create venv if it doesn't exist
  if [ -d "$TARGET_DIR/venv" ]; then
    echo "Virtual environment already exists"
  else
    echo "Creating Python virtual environment..."
    python3 -m venv "$TARGET_DIR/venv"
  fi
  
  # Activate venv
  source "$TARGET_DIR/venv/bin/activate"
  
  # Upgrade pip
  echo "Upgrading pip..."
  pip install --upgrade pip
  
  # Install base dependencies from the package
  echo "Installing LTX-Video package with inference dependencies..."
  cd "$TARGET_DIR"
  pip install -e '.[inference]'
  
  # Detect GPU backend and reinstall PyTorch if needed for CUDA
  echo "Detecting GPU backend..."
  
  if can-use-cuda; then
    echo "CUDA detected - reinstalling PyTorch with CUDA support..."
    # Uninstall CPU-only PyTorch that may have been installed
    pip uninstall -y torch torchvision torchaudio 2>/dev/null || true
    # Install PyTorch with CUDA 11.8 support
    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
  elif can-use-mps; then
    echo "MPS (Apple Silicon) detected - PyTorch already has MPS support"
    # MPS support is built into the standard PyTorch for macOS, no reinstall needed
  else
    echo "No GPU detected - using CPU-only PyTorch"
    # The default PyTorch from pip install -e '.[inference]' works for CPU
  fi
  
  echo ""
  echo "=== Installation Complete ==="
  echo "LTX-Video installed at: $TARGET_DIR"
  echo "Virtual environment: $TARGET_DIR/venv"
  echo ""
  echo "You can now run: ltx-video \"your prompt here\""
  
  exit 0
}

# Check for --install flag
for arg in "$@"; do
  if [ "$arg" = "--install" ]; then
    do_install
  fi
done

# Show help if requested
show_help() {
  cat << 'EOF'
ltx-video - Generate videos using LTX-Video

USAGE
  ltx-video [options] [prompt text...] [output.mp4]

ARGUMENT PARSING
  Text arguments       Concatenated with spaces to form the prompt
  -text                Single dash: added to negative prompt (things to avoid)
  --option             Double dash: passed directly to inference.py
  file.jpg             Existing file: used as reference image (image-to-video)
                       Multiple images are auto-distributed across frames
  output.mp4           Last argument ending in .mp4: output filename
                       Default: YYYY-MM-DD-HHMMSS-ltx-video.mp4

DEFAULT PARAMETERS
  Model:       ltxv-2b-0.9.8-distilled
  Resolution:  480x704
  Duration:    73 frames (3 seconds at 24 FPS)
  Frame Rate:  24 FPS

EXAMPLES
  ltx-video "a cat playing with yarn"
  ltx-video image.jpg "zoom out slowly"
  ltx-video start.jpg end.jpg "morph between images"
  ltx-video "spinning cube" cube.mp4
  ltx-video "beautiful landscape" -people -buildings
  ltx-video --num_frames 121 "longer video of a sunset"
  ltx-video --model ltxv-13b-0.9.8-distilled "high quality video"

WRAPPER OPTIONS
  --install            Install/update LTX-Video repo, create venv, install dependencies
  --model MODEL        Select model config (see AVAILABLE MODELS below)
  -m MODEL             Short form of --model

INFERENCE OPTIONS (passed to inference.py)
  --num_frames NUM     Number of frames (must be 8*N + 1: 9, 17, 25, 49, 73, 121)
  --height HEIGHT      Video height in pixels (must be divisible by 32)
  --width WIDTH        Video width in pixels (must be divisible by 32)
  --seed SEED          Random seed for reproducibility
  --offload_to_cpu     Enable CPU offloading for lower VRAM usage

EOF

  # List available models if LTX_DIR is found
  if [ -n "$LTX_DIR" ] && [ -d "$LTX_DIR/configs" ]; then
    echo "AVAILABLE MODELS"
    for config in "$LTX_DIR/configs"/*.yaml; do
      name=$(basename "$config" .yaml)
      if [ "$name" = "ltxv-2b-0.9.8-distilled" ]; then
        echo "  $name (default)"
      else
        echo "  $name"
      fi
    done
    echo ""
  fi

  echo "For full documentation, see: ~/bin/docs/ltx-video.md"
  exit 0
}

# Check for --help
for arg in "$@"; do
  if [ "$arg" = "--help" ] || [ "$arg" = "-h" ]; then
    show_help
  fi
done

# Determine LTX-Video repo location
if [ -d "$HOME/repos/LTX-Video" ]; then
  LTX_DIR="$HOME/repos/LTX-Video"
elif [ -d "$HOME/LTX-Video" ]; then
  LTX_DIR="$HOME/LTX-Video"
else
  echo "Error: LTX-Video repository not found in $HOME/repos/LTX-Video or $HOME/LTX-Video"
  exit 1
fi

# Default parameters
MODEL="ltxv-2b-0.9.8-distilled"
NUM_FRAMES=${NUM_FRAMES:-73}  # 3 seconds at 24 FPS (8*9 + 1)
FRAME_RATE=${FRAME_RATE:-24}
HEIGHT=${HEIGHT:-480}
WIDTH=${WIDTH:-704}
OUTPUT_FILE=""
PROMPT=""
NEGATIVE_PROMPT=""
REFERENCE_IMAGES=()
INFERENCE_ARGS=()

# ============================================================
# PASS 1: Process wrapper options and build filtered args list
# ============================================================
FILTERED_ARGS=()
SKIP_NEXT=false
ARGS=("$@")

for i in "${!ARGS[@]}"; do
  if [ "$SKIP_NEXT" = true ]; then
    SKIP_NEXT=false
    continue
  fi
  
  arg="${ARGS[$i]}"
  next_arg="${ARGS[$((i+1))]:-}"
  
  # Wrapper option: --model / -m
  if [ "$arg" = "--model" ] || [ "$arg" = "-m" ]; then
    if [ -n "$next_arg" ]; then
      MODEL="$next_arg"
      SKIP_NEXT=true
    else
      echo "Error: $arg requires a model name"
      exit 1
    fi
  # Add more wrapper options here in the future
  # elif [ "$arg" = "--some-option" ]; then
  #   ...
  else
    # Not a wrapper option, keep it for pass 2
    FILTERED_ARGS+=("$arg")
  fi
done

# ============================================================
# PASS 2: Process remaining arguments
# ============================================================
# Get last element (Bash 3.2 compatible - no negative indexing)
FILTERED_COUNT=${#FILTERED_ARGS[@]}
if [ "$FILTERED_COUNT" -gt 0 ]; then
  LAST_ARG="${FILTERED_ARGS[$((FILTERED_COUNT-1))]}"
else
  LAST_ARG=""
fi

for arg in "${FILTERED_ARGS[@]}"; do
  # Check if it's the last argument and ends with .mp4
  if [ "$arg" = "$LAST_ARG" ] && [[ "$arg" == *.mp4 ]]; then
    OUTPUT_FILE="$arg"
  # Check if it starts with single dash (negative prompt)
  elif [[ "$arg" =~ ^-[^-].* ]]; then
    # Remove leading dash and add to negative prompt
    NEGATIVE_TEXT="${arg#-}"
    if [ -z "$NEGATIVE_PROMPT" ]; then
      NEGATIVE_PROMPT="$NEGATIVE_TEXT"
    else
      NEGATIVE_PROMPT="$NEGATIVE_PROMPT, $NEGATIVE_TEXT"
    fi
  # Check if it starts with double dash (option for inference.py)
  elif [[ "$arg" =~ ^--.+ ]]; then
    INFERENCE_ARGS+=("$arg")
  # Check if it's an existing file (reference image)
  elif [ -f "$arg" ]; then
    REFERENCE_IMAGES+=("$arg")
  # Otherwise it's part of the prompt
  else
    if [ -z "$PROMPT" ]; then
      PROMPT="$arg"
    else
      PROMPT="$PROMPT $arg"
    fi
  fi
done

# Generate default output filename if not specified
if [ -z "$OUTPUT_FILE" ]; then
  TIMESTAMP=$(date +"%Y-%m-%d-%H%M%S")
  OUTPUT_FILE="${TIMESTAMP}-ltx-video.mp4"
fi

# Create temporary directory for inference output
TEMP_DIR=$(mktemp -d)
echo "Temporary output directory: $TEMP_DIR"

# Activate venv if present
if [ -d "$LTX_DIR/venv" ]; then
  source "$LTX_DIR/venv/bin/activate"
fi

# Check if height, width, num_frames, or frame_rate were passed in INFERENCE_ARGS
# If so, don't add them again (let the user's values take precedence)
HAS_HEIGHT=false
HAS_WIDTH=false
HAS_NUM_FRAMES=false
HAS_FRAME_RATE=false

for arg in "${INFERENCE_ARGS[@]}"; do
  if [ "$arg" = "--height" ]; then
    HAS_HEIGHT=true
  elif [ "$arg" = "--width" ]; then
    HAS_WIDTH=true
  elif [ "$arg" = "--num_frames" ]; then
    HAS_NUM_FRAMES=true
  elif [ "$arg" = "--frame_rate" ]; then
    HAS_FRAME_RATE=true
  fi
done

# Build command
CMD=(python "$LTX_DIR/inference.py")
CMD+=(--pipeline_config "$LTX_DIR/configs/${MODEL}.yaml")

# Only add defaults if not already specified in INFERENCE_ARGS
if [ "$HAS_HEIGHT" = false ]; then
  CMD+=(--height "$HEIGHT")
fi
if [ "$HAS_WIDTH" = false ]; then
  CMD+=(--width "$WIDTH")
fi
if [ "$HAS_NUM_FRAMES" = false ]; then
  CMD+=(--num_frames "$NUM_FRAMES")
fi
if [ "$HAS_FRAME_RATE" = false ]; then
  CMD+=(--frame_rate "$FRAME_RATE")
fi

CMD+=(--output_path "$TEMP_DIR")

# Add prompt if provided
if [ -n "$PROMPT" ]; then
  CMD+=(--prompt "$PROMPT")
fi

# Add negative prompt if provided
if [ -n "$NEGATIVE_PROMPT" ]; then
  CMD+=(--negative_prompt "$NEGATIVE_PROMPT")
fi

# Add reference images if provided
# Multiple images are distributed evenly across the frame range
if [ ${#REFERENCE_IMAGES[@]} -gt 0 ]; then
  NUM_IMAGES=${#REFERENCE_IMAGES[@]}
  
  # Calculate evenly distributed frame positions (spanning first to last frame)
  # For N images across F frames: position[i] = i * (F-1) / (N-1)
  # Examples:
  #   1 image, 25 frames: frame 0
  #   2 images, 25 frames: frames 0, 24
  #   3 images, 25 frames: frames 0, 12, 24
  #   4 images, 25 frames: frames 0, 8, 16, 24
  FRAME_POSITIONS=()
  i=0
  while [ $i -lt $NUM_IMAGES ]; do
    if [ $NUM_IMAGES -eq 1 ]; then
      FRAME_POS=0
    else
      # Calculate position: i * (NUM_FRAMES - 1) / (NUM_IMAGES - 1)
      # Using integer division, spans from frame 0 to last frame
      FRAME_POS=$(( i * (NUM_FRAMES - 1) / (NUM_IMAGES - 1) ))
    fi
    FRAME_POSITIONS+=("$FRAME_POS")
    i=$((i + 1))
  done
  
  echo "Reference images: ${REFERENCE_IMAGES[*]}"
  echo "Frame positions: ${FRAME_POSITIONS[*]}"
  
  # Pass as space-separated arguments (not comma-separated)
  CMD+=(--conditioning_media_paths "${REFERENCE_IMAGES[@]}")
  CMD+=(--conditioning_start_frames "${FRAME_POSITIONS[@]}")
fi

# Add extra arguments
if [ ${#INFERENCE_ARGS[@]} -gt 0 ]; then
  CMD+=("${INFERENCE_ARGS[@]}")
fi

# Execute with timing
echo "Running: ${CMD[*]}"
START_TIME=$(date +%s)
"${CMD[@]}"
END_TIME=$(date +%s)
ELAPSED=$((END_TIME - START_TIME))

# Find the generated video file and move it
GENERATED_VIDEO=$(find "$TEMP_DIR" -name "*.mp4" -type f | head -n 1)

if [ -n "$GENERATED_VIDEO" ]; then
  mv "$GENERATED_VIDEO" "$OUTPUT_FILE"
  echo "Video saved to: $OUTPUT_FILE"
  echo "Generation time: ${ELAPSED} seconds"
  # Clean up temp directory
  rm -rf "$TEMP_DIR"
else
  echo "Error: No video file generated in $TEMP_DIR"
  exit 1
fi
